{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOlA3DmMZkVx"
      },
      "source": [
        "# Sentiment Analysis Ensemble Labeling using VADER and TextBlob\n",
        "\n",
        "This notebook implements an ensemble approach to sentiment labeling by combining predictions from VADER and TextBlob sentiment analyzers. The process includes:\n",
        "\n",
        "1. **Dual Annotation**: Apply both VADER and TextBlob to generate sentiment labels\n",
        "2. **Agreement Analysis**: Calculate Cohen's Kappa to measure inter-annotator agreement\n",
        "3. **Ensemble Creation**: Use majority voting to create final consensus labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plXrSmnttCdp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "file_path = '/content/GenAI in education Dataset - latest cleaned.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "data = data.drop(columns=['link'])\n",
        "\n",
        "vader_analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def vader_sentiment(text):\n",
        "    score = vader_analyzer.polarity_scores(text)['compound']\n",
        "    if score >= 0.05:\n",
        "        return 'positive'\n",
        "    elif score <= -0.05:\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'neutral'\n",
        "\n",
        "def textblob_sentiment(text):\n",
        "    polarity = TextBlob(text).sentiment.polarity\n",
        "    if polarity > 0:\n",
        "        return 'positive'\n",
        "    elif polarity < 0:\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'neutral'\n",
        "\n",
        "\n",
        "data['vader_label'] = data['text_clean'].apply(vader_sentiment)\n",
        "data['textblob_label'] = data['text_clean'].apply(textblob_sentiment)\n",
        "\n",
        "# Save comparison CSV\n",
        "data.to_csv('sentiment_labels_comparison.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Dual Sentiment Annotation\n",
        "\n",
        "We'll apply two different sentiment analysis approaches to create independent labels:\n",
        "\n",
        "### VADER (Valence Aware Dictionary and sEntiment Reasoner)\n",
        "- **Compound Score Thresholds**:\n",
        "  - Positive: ≥ 0.05\n",
        "  - Negative: ≤ -0.05  \n",
        "  - Neutral: between -0.05 and 0.05\n",
        "\n",
        "### TextBlob\n",
        "- **Polarity Score Thresholds**:\n",
        "  - Positive: > 0\n",
        "  - Negative: < 0\n",
        "  - Neutral: = 0\n",
        "\n",
        "Both methods will analyze the `text_clean` field to generate sentiment classifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzxnPUCY8dW7",
        "outputId": "cb118335-3c31-4ed0-f18e-82e2cdd19f52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cohen's Kappa between VADER and TextBlob labels: 0.307\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Load the CSV with labels\n",
        "file_path = 'sentiment_labels_comparison.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Drop rows with missing values in the sentiment label columns\n",
        "data_cleaned = data.dropna(subset=['vader_label', 'textblob_label'])\n",
        "\n",
        "# Calculate Cohen's Kappa\n",
        "kappa = cohen_kappa_score(data_cleaned['vader_label'], data_cleaned['textblob_label'])\n",
        "\n",
        "print(f\"Cohen's Kappa between VADER and TextBlob labels: {kappa:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Inter-Annotator Agreement Analysis\n",
        "\n",
        "Now we'll measure the agreement between VADER and TextBlob using **Cohen's Kappa coefficient**.\n",
        "\n",
        "### Cohen's Kappa Interpretation:\n",
        "- **κ < 0.20**: Poor agreement\n",
        "- **0.20 ≤ κ < 0.40**: Fair agreement  \n",
        "- **0.40 ≤ κ < 0.60**: Moderate agreement\n",
        "- **0.60 ≤ κ < 0.80**: Good agreement\n",
        "- **κ ≥ 0.80**: Very good agreement\n",
        "\n",
        "This metric helps us understand how consistently the two methods classify sentiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZjMsrGMo5_R"
      },
      "source": [
        "## Step 3: Ensemble Labeling Strategy\n",
        "\n",
        "We'll create consensus labels using a **majority voting approach** with the following logic:\n",
        "\n",
        "### Voting Rules:\n",
        "1. **Unanimous Positive**: Both VADER and TextBlob agree on \"positive\" → **positive**\n",
        "2. **Unanimous Negative**: Both VADER and TextBlob agree on \"negative\" → **negative**  \n",
        "3. **Disagreement**: Any other combination (including neutral) → **negative** (conservative approach)\n",
        "\n",
        "This conservative approach ensures that only clearly positive sentiments are labeled as positive, while ambiguous cases default to negative. This helps create a more reliable training dataset for subsequent machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cmpUm4qo5lt"
      },
      "outputs": [],
      "source": [
        "def ensemble_vote(row):\n",
        "    votes = [row['vader_label'], row['textblob_label']]\n",
        "\n",
        "    # Count votes\n",
        "    vote_counts = Counter(votes)\n",
        "\n",
        "    vote_counts = Counter(votes)\n",
        "\n",
        "    if vote_counts['positive'] == 2:\n",
        "        return 'positive'\n",
        "    elif vote_counts['negative'] == 2:\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'negative'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRglK1lFpR8P"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "df = pd.read_csv(\"/content/GenAI in education Dataset - sentiment_labels_comparison.csv\")\n",
        "\n",
        "# Apply ensemble voting\n",
        "df['ensemble_label'] = df.apply(ensemble_vote, axis=1)\n",
        "df.to_csv(\"ensemble_sentiment_labels.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Apply Ensemble Voting and Save Results\n",
        "\n",
        "Finally, we'll apply the ensemble voting function to create the final consensus labels and save the annotated dataset for use in downstream machine learning tasks.\n",
        "\n",
        "### Output Dataset\n",
        "- **File**: `ensemble_sentiment_labels.csv`\n",
        "- **New Column**: `ensemble_label` (positive/negative)\n",
        "- **Purpose**: Training data for supervised sentiment analysis models\n",
        "\n",
        "This ensemble-labeled dataset will serve as the ground truth for training more sophisticated sentiment analysis models like BERT or DistilBERT."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
