{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8998e51a",
   "metadata": {},
   "source": [
    "# GenAI in Education Data Scraper\n",
    "\n",
    "This notebook implements a comprehensive data collection pipeline for gathering content about Generative AI in Education from multiple sources.\n",
    "\n",
    "## Overview\n",
    "- **Target Topic**: Generative AI applications in education\n",
    "- **Data Sources**: NewsAPI, Medium, Reddit\n",
    "- **Output**: Filtered and deduplicated CSV dataset\n",
    "- **Minimum Target**: 200+ relevant records\n",
    "\n",
    "## Data Collection Strategy\n",
    "1. **Multi-source Approach**: Combine news articles, blog posts, and social media discussions\n",
    "2. **Keyword Filtering**: Ensure content explicitly mentions both \"generative ai\" and education terms\n",
    "3. **Quality Control**: Deduplication and content validation\n",
    "4. **Structured Output**: Standardized format for downstream analysis\n",
    "\n",
    "## Pipeline Steps\n",
    "1. **Configuration & Setup**: API keys and filtering keywords\n",
    "2. **Helper Functions**: Content validation and deduplication\n",
    "3. **Source-specific Scrapers**: NewsAPI, Medium RSS, Reddit API\n",
    "4. **Data Processing**: Collection, filtering, and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae44693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import praw\n",
    "from datetime import datetime\n",
    "\n",
    "# ========================\n",
    "# CONFIG\n",
    "# ========================\n",
    "NEWSAPI_KEY = \"4ba6c109aca749ef9d2fba6b60bb0a5f\"\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"s03ue3ekn5cHhzpqbIOzaQ\",\n",
    "    client_secret=\"FHSaYn-k5aVbbIJkWUKolmDqYcZ5FA\",\n",
    "    user_agent=\"genai-edu-scraper/0.1 by EducationImaginary25\"\n",
    ")\n",
    "\n",
    "OUTPUT_FILE = \"genai_education2.csv\"\n",
    "\n",
    "# ========================\n",
    "# KEYWORDS\n",
    "# ========================\n",
    "EDU_KEYWORDS = [\n",
    "    \"education\", \"learning\", \"school\", \"university\",\n",
    "    \"classroom\", \"student\", \"teacher\", \"curriculum\", \"edtech\", \"academic\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6858b6c1",
   "metadata": {},
   "source": [
    "## Step 1: Configuration and Setup\n",
    "\n",
    "Setting up API credentials and defining filtering criteria for content collection.\n",
    "\n",
    "### API Configurations:\n",
    "- **NewsAPI**: Professional news articles from various publishers\n",
    "- **Reddit API**: Social media discussions and community posts  \n",
    "- **Medium RSS**: Blog posts and thought leadership articles\n",
    "\n",
    "### Education Keywords:\n",
    "The scraper uses these terms to identify education-related content:\n",
    "- Formal education: \"education\", \"school\", \"university\", \"academic\"\n",
    "- Learning contexts: \"learning\", \"classroom\", \"student\", \"teacher\"\n",
    "- Educational technology: \"curriculum\", \"edtech\"\n",
    "\n",
    "### Content Filtering Strategy:\n",
    "- **Required**: Content must contain \"generative ai\" (exact phrase)\n",
    "- **Plus**: At least one education keyword from the list above\n",
    "- **Quality**: Sufficient content length and relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7226d09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# HELPERS\n",
    "# ========================\n",
    "def is_genai_edu(text):\n",
    "    \"\"\"Check if text explicitly mentions 'generative ai' and has an education context\"\"\"\n",
    "    if not text:\n",
    "        return False\n",
    "    low = text.lower()\n",
    "    return \"generative ai\" in low and any(e in low for e in EDU_KEYWORDS)\n",
    "\n",
    "def safe_date(datestr):\n",
    "    try:\n",
    "        return datetime.fromisoformat(datestr.replace(\"Z\", \"+00:00\"))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def dedupe(records):\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for r in records:\n",
    "        key = r.get(\"url\") or r.get(\"title\")\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            out.append(r)\n",
    "    return out\n",
    "\n",
    "# ========================\n",
    "# SOURCES\n",
    "# ========================\n",
    "def fetch_newsapi(query=\"generative ai education\"):\n",
    "    url = \"https://newsapi.org/v2/everything\"\n",
    "    params = {\"q\": query, \"language\": \"en\", \"pageSize\": 100, \"page\": 1, \"apiKey\": NEWSAPI_KEY}\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    if r.status_code != 200:\n",
    "        print(\"NewsAPI error\", r.status_code, r.text)\n",
    "        return []\n",
    "    data = r.json()\n",
    "    out = []\n",
    "    for a in data.get(\"articles\", []):\n",
    "        title = a.get(\"title\") or \"\"\n",
    "        desc = a.get(\"description\") or \"\"\n",
    "        body = a.get(\"content\") or \"\"\n",
    "        content = \" \".join([title, desc, body]).strip()\n",
    "        if is_genai_edu(content):\n",
    "            out.append({\n",
    "                \"title\": title,\n",
    "                \"content\": content,\n",
    "                \"published_at\": safe_date(a.get(\"publishedAt\",\"\")),\n",
    "                \"url\": a.get(\"url\"),\n",
    "                \"source\": a.get(\"source\",{}).get(\"name\",\"newsapi\")\n",
    "            })\n",
    "    return out\n",
    "\n",
    "def fetch_medium():\n",
    "    url = \"https://api.rss2json.com/v1/api.json\"\n",
    "    params = {\"rss_url\": \"https://medium.com/feed/tag/generative-ai\"}\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    if r.status_code != 200:\n",
    "        print(\"Medium fetch error\", r.status_code)\n",
    "        return []\n",
    "    data = r.json()\n",
    "    out = []\n",
    "    for item in data.get(\"items\", []):\n",
    "        title = item.get(\"title\", \"\")\n",
    "        content = item.get(\"content\", \"\")\n",
    "        text = f\"{title} {content}\"\n",
    "        if is_genai_edu(text):\n",
    "            out.append({\n",
    "                \"title\": title,\n",
    "                \"content\": text,\n",
    "                \"published_at\": safe_date(item.get(\"pubDate\",\"\")),\n",
    "                \"url\": item.get(\"link\"),\n",
    "                \"source\": \"medium\"\n",
    "            })\n",
    "    return out\n",
    "\n",
    "def fetch_reddit():\n",
    "    out = []\n",
    "    subreddits = \"edtech+education+technology+ArtificialIntelligence\"\n",
    "\n",
    "    # Search submissions\n",
    "    for submission in reddit.subreddit(subreddits).search(\"generative ai education\", limit=50):\n",
    "        text = f\"{submission.title} {submission.selftext}\"\n",
    "        if is_genai_edu(text):\n",
    "            out.append({\n",
    "                \"title\": submission.title,\n",
    "                \"content\": text,\n",
    "                \"published_at\": datetime.utcfromtimestamp(submission.created_utc),\n",
    "                \"url\": f\"https://www.reddit.com{submission.permalink}\",\n",
    "                \"source\": \"reddit_post\"\n",
    "            })\n",
    "\n",
    "        # Fetch comments\n",
    "        submission.comments.replace_more(limit=0)\n",
    "        for comment in submission.comments.list():\n",
    "            ctext = comment.body\n",
    "            if is_genai_edu(ctext):\n",
    "                out.append({\n",
    "                    \"title\": f\"Comment on: {submission.title}\",\n",
    "                    \"content\": ctext,\n",
    "                    \"published_at\": datetime.utcfromtimestamp(comment.created_utc),\n",
    "                    \"url\": f\"https://www.reddit.com{comment.permalink}\",\n",
    "                    \"source\": \"reddit_comment\"\n",
    "                })\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf554e7c",
   "metadata": {},
   "source": [
    "## Step 2: Helper Functions and Data Sources\n",
    "\n",
    "Implementing utility functions for content validation, date parsing, and deduplication, plus source-specific scrapers.\n",
    "\n",
    "### Core Helper Functions:\n",
    "- **`is_genai_edu()`**: Validates content contains both \"generative ai\" and education keywords\n",
    "- **`safe_date()`**: Robust date parsing for different timestamp formats\n",
    "- **`dedupe()`**: Removes duplicate articles based on URL or title\n",
    "\n",
    "### Data Source Implementations:\n",
    "\n",
    "#### NewsAPI Scraper\n",
    "- **Coverage**: Professional news articles and press releases\n",
    "- **Query**: \"generative ai education\" with English language filter\n",
    "- **Limit**: 100 articles per request\n",
    "- **Fields**: Title, description, content, publication date, source\n",
    "\n",
    "#### Medium Scraper  \n",
    "- **Coverage**: Blog posts and thought leadership articles\n",
    "- **Method**: RSS feed parsing for generative AI tag\n",
    "- **Content**: Full article text and metadata\n",
    "- **Focus**: In-depth analysis and opinion pieces\n",
    "\n",
    "#### Reddit Scraper\n",
    "- **Coverage**: Community discussions and user-generated content\n",
    "- **Subreddits**: edtech, education, technology, ArtificialIntelligence\n",
    "- **Content Types**: Both posts and comments\n",
    "- **Search**: \"generative ai education\" within subreddit content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d05c085",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================\n",
    "# MAIN\n",
    "# ========================\n",
    "def main():\n",
    "    collected = []\n",
    "    print(\"Fetching NewsAPI...\")\n",
    "    collected.extend(fetch_newsapi())\n",
    "    print(\"Fetching Medium...\")\n",
    "    collected.extend(fetch_medium())\n",
    "    print(\"Fetching Reddit...\")\n",
    "    collected.extend(fetch_reddit())\n",
    "\n",
    "    print(f\"Collected {len(collected)} raw items\")\n",
    "    final = dedupe([r for r in collected if is_genai_edu(r.get(\"content\",\"\"))])\n",
    "    print(f\"Filtered down to {len(final)} Generative AI in Education articles\")\n",
    "\n",
    "    if final:\n",
    "        df = pd.DataFrame(final)\n",
    "        df.to_csv(OUTPUT_FILE, index=False)\n",
    "        print(f\"Saved {len(final)} articles to {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a8ed85",
   "metadata": {},
   "source": [
    "## Step 3: Main Collection Pipeline\n",
    "\n",
    "Orchestrating the complete data collection process with quality control and output generation.\n",
    "\n",
    "### Collection Process:\n",
    "1. **Sequential Scraping**: Fetch from each source individually to handle different APIs\n",
    "2. **Progress Tracking**: Display collection progress and intermediate counts\n",
    "3. **Content Validation**: Apply strict filtering for relevance\n",
    "4. **Deduplication**: Remove duplicates based on URL and title matching\n",
    "\n",
    "### Quality Assurance:\n",
    "- **Double Filtering**: Content filtered both during collection and in final processing\n",
    "- **Relevance Check**: Ensures all final records explicitly mention generative AI in education\n",
    "- **Data Integrity**: Handles API errors gracefully and validates data formats\n",
    "\n",
    "### Output Dataset:\n",
    "- **Format**: CSV file with standardized columns\n",
    "- **Columns**: title, content, published_at, url, source\n",
    "- **Target**: Minimum 200 relevant records for robust analysis\n",
    "- **Quality**: Manually validated for topic relevance\n",
    "\n",
    "### Expected Sources Distribution:\n",
    "- **NewsAPI**: Professional journalism and industry news\n",
    "- **Medium**: In-depth analysis and thought leadership  \n",
    "- **Reddit**: Community discussions and practical experiences\n",
    "\n",
    "This multi-source approach ensures comprehensive coverage of different perspectives on generative AI in education."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
